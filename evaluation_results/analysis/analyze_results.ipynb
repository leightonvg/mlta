{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to reproduce the results analysis, e.g. statistical tests and plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook re-creates the statistical tests and figures based on the obtained results -- which are present in this directory. The kernel density and run time figures are created as actual figures (.jpg files), the CD diagrams are outputted as Latex's tikz code, hence to actually visualize the figure you must run it in Latex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains both the time-restricted analysis on the full and the partial OpenML-CC18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from critdd import Diagram\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the already obtained results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the metadatabase top 25 in a new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdbase = pd.read_csv(\"metadatabase_aggregated.csv\")\n",
    "\n",
    "def process_logsname_string(s):\n",
    "    if \"AsynchronousSuccessiveHalving\" in s:\n",
    "        return \"AsynchronousSuccessiveHalving\"\n",
    "    elif \"AsyncEA\" in s:\n",
    "        return \"AsyncEA\"\n",
    "    elif \"RandomSearch\" in s:\n",
    "        return \"RandomSearch\"\n",
    "\n",
    "df_mdbase[\"logs_name\"] = df_mdbase[\"logs_name\"].apply(lambda x : process_logsname_string(x))\n",
    "\n",
    "logs_names = df_mdbase[\"logs_name\"].unique().tolist()\n",
    "dataset_ids = df_mdbase[\"dataset_id\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_select = 25\n",
    "df_total = None\n",
    "\n",
    "for logs_name in logs_names:\n",
    "    for did in dataset_ids:\n",
    "        missing_entry_val = min(df_mdbase[(df_mdbase[\"dataset_id\"] == did) & (df_mdbase[\"score\"] < 1000000)][\"score\"]) - 0.01\n",
    "        df_learner_dataset = df_mdbase[(df_mdbase[\"logs_name\"] == logs_name) & (df_mdbase[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset.reset_index(inplace=True, drop=True)\n",
    "        df_top25 = df_learner_dataset.iloc[[i for i in range(0, min(n_select, len(df_learner_dataset)))]]\n",
    "        if df_total is None:\n",
    "            df_total = df_top25\n",
    "        else:\n",
    "            df_total = pd.concat([df_total, df_top25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total[[\"logs_name\", \"dataset_id\", \"score\"]]\n",
    "df_total.to_csv(\"mdbase_top25.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mdbase data in correct format -- full OpenML18CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the CD diagram I should still pick only 1 evaluation per meta-learner-dataset\n",
    "\n",
    "df_mdbase = pd.read_csv(\"../results_backup/metadatabase_aggregated.csv\")\n",
    "\n",
    "def process_logsname_string(s):\n",
    "    if \"AsynchronousSuccessiveHalving\" in s:\n",
    "        return \"AsynchronousSuccessiveHalving\"\n",
    "    elif \"AsyncEA\" in s:\n",
    "        return \"AsyncEA\"\n",
    "    elif \"RandomSearch\" in s:\n",
    "        return \"RandomSearch\"\n",
    "\n",
    "df_mdbase[\"logs_name\"] = df_mdbase[\"logs_name\"].apply(lambda x : process_logsname_string(x))\n",
    "\n",
    "logs_names = df_mdbase[\"logs_name\"].unique().tolist()\n",
    "dataset_ids = df_mdbase[\"dataset_id\"].unique().tolist()\n",
    "\n",
    "n_select = 5 # number to select from\n",
    "\n",
    "idx_to_keep = []\n",
    "rows_to_add = []\n",
    "for logs_name in logs_names:\n",
    "    for did in dataset_ids:\n",
    "        missing_entry_val = min(df_mdbase[(df_mdbase[\"dataset_id\"] == did) & (df_mdbase[\"score\"] < 1000000)][\"score\"]) - 0.01\n",
    "        df_learner_dataset = df_mdbase[(df_mdbase[\"logs_name\"] == logs_name) & (df_mdbase[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset.reset_index(inplace=True)\n",
    "        if len(df_learner_dataset) == 0:\n",
    "            rows_to_add.append([logs_name, did, missing_entry_val])\n",
    "        else:\n",
    "            idx_max = df_learner_dataset.iloc[[i for i in range(0, min(n_select, len(df_learner_dataset)))]][\"score\"].idxmax()\n",
    "            if not isinstance(idx_max, np.int64):  # no valid entries for the combo\n",
    "                rows_to_add.append([logs_name, did, missing_entry_val])\n",
    "            else:\n",
    "                idx_to_keep.append(df_learner_dataset.iloc[idx_max][\"index\"])\n",
    "\n",
    "# select idx and add rows for missing entries\n",
    "df_top5 = df_mdbase.iloc[idx_to_keep][[\"logs_name\", \"dataset_id\", \"score\"]]\n",
    "n = len(df_mdbase)\n",
    "for row in rows_to_add:\n",
    "    df_top5.loc[n] = row\n",
    "    n += 1\n",
    "\n",
    "df_top5.to_csv(\"mdbase_baselines_processed.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get mdbase data in correct format -- partial OpenML18CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the CD diagram I should still pick only 1 evaluation per meta-learner-dataset\n",
    "\n",
    "df_mdbase = pd.read_csv(\"../results_backup/metadatabase_aggregated.csv\")\n",
    "openmlcc18_subset_dids = [59, 47, 31, 71, 25, 26, 29, 32, 28, 33, 60, 58, 21, 39, 30, 70, 4, 6, 8, 65, 3, 49, 64, 52, 46, 19, 56, 66, 42, 45, 43, 24, 35, 53, 27, 63, 67, 68]\n",
    "df_mdbase = df_mdbase[df_mdbase[\"dataset_id\"].isin(openmlcc18_subset_dids)]\n",
    "df_mdbase.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def process_logsname_string(s):\n",
    "    if \"AsynchronousSuccessiveHalving\" in s:\n",
    "        return \"AsynchronousSuccessiveHalving\"\n",
    "    elif \"AsyncEA\" in s:\n",
    "        return \"AsyncEA\"\n",
    "    elif \"RandomSearch\" in s:\n",
    "        return \"RandomSearch\"\n",
    "\n",
    "df_mdbase[\"logs_name\"] = df_mdbase[\"logs_name\"].apply(lambda x : process_logsname_string(x))\n",
    "\n",
    "logs_names = df_mdbase[\"logs_name\"].unique().tolist()\n",
    "dataset_ids = df_mdbase[\"dataset_id\"].unique().tolist()\n",
    "\n",
    "n_select = 5 # number to select from\n",
    "\n",
    "idx_to_keep = []\n",
    "rows_to_add = []\n",
    "for logs_name in logs_names:\n",
    "    for did in dataset_ids:\n",
    "        missing_entry_val = min(df_mdbase[(df_mdbase[\"dataset_id\"] == did) & (df_mdbase[\"score\"] < 1000000)][\"score\"]) - 0.01\n",
    "        df_learner_dataset = df_mdbase[(df_mdbase[\"logs_name\"] == logs_name) & (df_mdbase[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset.reset_index(inplace=True)\n",
    "        if len(df_learner_dataset) == 0:\n",
    "            rows_to_add.append([logs_name, did, missing_entry_val])\n",
    "        else:\n",
    "            idx_max = df_learner_dataset.iloc[[i for i in range(0, min(n_select, len(df_learner_dataset)))]][\"score\"].idxmax()\n",
    "            if not isinstance(idx_max, np.int64):  # no valid entries for the combo\n",
    "                rows_to_add.append([logs_name, did, missing_entry_val])\n",
    "            else:\n",
    "                idx_to_keep.append(df_learner_dataset.iloc[idx_max][\"index\"])\n",
    "\n",
    "# select idx and add rows for missing entries\n",
    "df_top5 = df_mdbase.iloc[idx_to_keep][[\"logs_name\", \"dataset_id\", \"score\"]]\n",
    "n = len(df_mdbase)\n",
    "for row in rows_to_add:\n",
    "    df_top5.loc[n] = row\n",
    "    n += 1\n",
    "\n",
    "df_top5.to_csv(\"mdbase_baselines_processed_partial_openmlcc18.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get meta-learner data in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the CD diagram I should still pick only 1 evaluation per meta-learner-dataset\n",
    "df_all = pd.read_csv(\"time_restricted_results_no_dataset2vec.csv\")\n",
    "\n",
    "metalearners = df_all[\"metalearner_name\"].unique().tolist()\n",
    "dataset_ids = df_all[\"dataset_id\"].unique().tolist()\n",
    "\n",
    "n_select = 5 # number to select from\n",
    "\n",
    "idx_to_keep = []\n",
    "rows_to_add = []\n",
    "for metalearner in metalearners:\n",
    "    for did in dataset_ids:\n",
    "        # filter out nan with selection: < 1000000\n",
    "        missing_entry_val = min(df_all[(df_all[\"dataset_id\"] == did) & (df_all[\"neg_log_loss\"] < 1000000)][\"neg_log_loss\"]) - 0.01\n",
    "        df_learner_dataset = df_all[(df_all[\"metalearner_name\"] == metalearner) & (df_all[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset.reset_index(inplace=True)\n",
    "        if len(df_learner_dataset) == 0:\n",
    "            rows_to_add.append([metalearner, did, missing_entry_val])\n",
    "        else:\n",
    "            idx_max = df_learner_dataset.iloc[[i for i in range(0, min(n_select, len(df_learner_dataset)))]][\"neg_log_loss\"].idxmax()\n",
    "            if not isinstance(idx_max, np.int64):  # no valid entries for the combo\n",
    "                rows_to_add.append([metalearner, did, missing_entry_val])\n",
    "            else:\n",
    "                idx_to_keep.append(df_learner_dataset.iloc[idx_max][\"index\"])\n",
    "\n",
    "# select idx and add rows for missing entries\n",
    "df_top5 = df_all.iloc[idx_to_keep]\n",
    "n = len(df_all)\n",
    "for row in rows_to_add:\n",
    "    df_top5.loc[n] = row\n",
    "    n += 1\n",
    "\n",
    "df_top5.to_csv(\"time_restricted_results_no_dataset2vec_processed.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get meta-learner data in correct format -- subset OpenML-CC18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the CD diagram I should still pick only 1 evaluation per meta-learner-dataset\n",
    "df_all = pd.read_csv(\"time_restricted_results.csv\")\n",
    "openmlcc18_subset_dids = [59, 47, 31, 71, 25, 26, 29, 32, 28, 33, 60, 58, 21, 39, 30, 70, 4, 6, 8, 65, 3, 49, 64, 52, 46, 19, 56, 66, 42, 45, 43, 24, 35, 53, 27, 63, 67, 68]\n",
    "df_all = df_all[df_all[\"dataset_id\"].isin(openmlcc18_subset_dids)]\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "metalearners = df_all[\"metalearner_name\"].unique().tolist()\n",
    "dataset_ids = df_all[\"dataset_id\"].unique().tolist()\n",
    "\n",
    "n_select = 5 # number to select from\n",
    "\n",
    "idx_to_keep = []\n",
    "rows_to_add = []\n",
    "for metalearner in metalearners:\n",
    "    for did in dataset_ids:\n",
    "        # filter out nan with selection: < 1000000\n",
    "        missing_entry_val = min(df_all[(df_all[\"dataset_id\"] == did) & (df_all[\"neg_log_loss\"] < 1000000)][\"neg_log_loss\"]) - 0.01\n",
    "        df_learner_dataset = df_all[(df_all[\"metalearner_name\"] == metalearner) & (df_all[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset.reset_index(inplace=True)\n",
    "        if len(df_learner_dataset) == 0:\n",
    "            rows_to_add.append([metalearner, did, missing_entry_val])\n",
    "        else:\n",
    "            idx_max = df_learner_dataset.iloc[[i for i in range(0, min(n_select, len(df_learner_dataset)))]][\"neg_log_loss\"].idxmax()\n",
    "            if not isinstance(idx_max, np.int64):  # no valid entries for the combo\n",
    "                rows_to_add.append([metalearner, did, missing_entry_val])\n",
    "            else:\n",
    "                idx_to_keep.append(df_learner_dataset.iloc[idx_max][\"index\"])\n",
    "\n",
    "# select idx and add rows for missing entries\n",
    "df_top5 = df_all.iloc[idx_to_keep]\n",
    "n = len(df_all)\n",
    "for row in rows_to_add:\n",
    "    df_top5.loc[n] = row\n",
    "    n += 1\n",
    "\n",
    "df_top5.to_csv(\"time_restricted_results_processed_partial_openmlcc18.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friedman test, before CD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full OpenML-CC18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "results_df = pd.read_csv(\"time_restricted_results_no_dataset2vec_processed_with_baselines.csv\")\n",
    "results_df[\"neg_log_loss\"] = results_df[\"neg_log_loss\"].astype(float)\n",
    "groups = []\n",
    "\n",
    "for metalearner in list(results_df[\"metalearner_name\"].value_counts().index):\n",
    "    partial_df = results_df[results_df[\"metalearner_name\"] == metalearner]\n",
    "    groups.append(list(partial_df[\"neg_log_loss\"]))\n",
    "\n",
    "#perform Friedman Test\n",
    "stats.friedmanchisquare(*groups)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for OpenML-CC18 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "results_df = pd.read_csv(\"time_restricted_results_processed_partial_openmlcc18_with_baselines.csv\")\n",
    "results_df[\"neg_log_loss\"] = results_df[\"neg_log_loss\"].astype(float)\n",
    "groups = []\n",
    "\n",
    "for metalearner in list(results_df[\"metalearner_name\"].value_counts().index):\n",
    "    partial_df = results_df[results_df[\"metalearner_name\"] == metalearner]\n",
    "    groups.append(list(partial_df[\"neg_log_loss\"]))\n",
    "\n",
    "#perform Friedman Test\n",
    "stats.friedmanchisquare(*groups)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CD Diagrams for time-restricted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from critdd import Diagram\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"time_restricted_results_no_dataset2vec_processed_with_baselines.csv\").pivot(\n",
    "    index = \"dataset_id\",\n",
    "    columns = \"metalearner_name\",\n",
    "    values = \"neg_log_loss\"\n",
    ")\n",
    "\n",
    "# create a CD diagram from the Pandas DataFrame\n",
    "diagram = Diagram(\n",
    "    df.to_numpy(),\n",
    "    treatment_names = df.columns,\n",
    "    maximize_outcome = True\n",
    ")\n",
    "\n",
    "# inspect average ranks and groups of statistically indistinguishable treatments\n",
    "diagram.average_ranks # the average rank of each treatment\n",
    "diagram.get_groups(alpha=.05, adjustment=\"bonferroni\")\n",
    "\n",
    "# export the diagram to a file\n",
    "diagram.to_file(\n",
    "    \"cd_full_openmlcc18.tex\",\n",
    "    alpha = .05,\n",
    "    adjustment = \"bonferroni\",\n",
    "    reverse_x = True,\n",
    "    axis_options = {\"title\": \"critdd\"},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for OpenML-CC18 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from critdd import Diagram\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"time_restricted_results_processed_partial_openmlcc18_with_baselines.csv\").pivot(\n",
    "    index = \"dataset_id\",\n",
    "    columns = \"metalearner_name\",\n",
    "    values = \"neg_log_loss\"\n",
    ")\n",
    "\n",
    "# create a CD diagram from the Pandas DataFrame\n",
    "diagram = Diagram(\n",
    "    df.to_numpy(),\n",
    "    treatment_names = df.columns,\n",
    "    maximize_outcome = True\n",
    ")\n",
    "\n",
    "# inspect average ranks and groups of statistically indistinguishable treatments\n",
    "diagram.average_ranks # the average rank of each treatment\n",
    "diagram.get_groups(alpha=.05, adjustment=\"bonferroni\")\n",
    "\n",
    "# export the diagram to a file\n",
    "diagram.to_file(\n",
    "    \"cd_partial_openmlcc18.tex\",\n",
    "    alpha = .05,\n",
    "    adjustment = \"bonferroni\",\n",
    "    reverse_x = True,\n",
    "    axis_options = {\"title\": \"critdd\"},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the Average Regret and SD of recommended configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"time_restricted_results.csv\")\n",
    "metalearner_subset = [mtl_name for mtl_name in df_all[\"metalearner_name\"].unique().tolist() if not \"2v\" in mtl_name]\n",
    "df_all = df_all[df_all[\"metalearner_name\"].isin(metalearner_subset)]\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all.to_csv(\"time_restricted_results_no_dataset2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out getting the potential score\n",
    "df = pd.read_csv(\"time_restricted_results_no_dataset2vec.csv\")\n",
    "\n",
    "metalearners = df[\"metalearner_name\"].unique().tolist()\n",
    "dataset_ids = df[\"dataset_id\"].unique().tolist()\n",
    "\n",
    "n_recommendation = 25\n",
    "\n",
    "def regret(score):\n",
    "    if score < 100000: # filter out NAN\n",
    "        return abs(max_score - score) / abs(min_score - max_score)\n",
    "    return 1\n",
    "\n",
    "idx_to_keep = []\n",
    "rows_to_add = []\n",
    "metalearner_potential = {}\n",
    "metalearner_std = {}\n",
    "for metalearner in metalearners:\n",
    "    metalearner_dataset_potentials = []\n",
    "    metalearner_dataset_potentials_std = []\n",
    "    for did in dataset_ids:\n",
    "        # filter out nan with selection: < 1000000\n",
    "        missing_entry_val = min(df[(df[\"dataset_id\"] == did) & (df[\"neg_log_loss\"] < 1000000)][\"neg_log_loss\"]) - 0.01\n",
    "        df_did = df[df[\"dataset_id\"] == did]\n",
    "        did_losses_without_nan = [df_did.iloc[i][\"neg_log_loss\"] for i in range(0, len(df_did)) if df_did.iloc[i][\"neg_log_loss\"] < 1_000_000] # comparison to exclude nans\n",
    "        min_score = min(did_losses_without_nan)  \n",
    "        max_score = max(did_losses_without_nan) \n",
    "\n",
    "        df_learner_dataset = df[(df[\"metalearner_name\"] == metalearner) & (df[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset[\"potential\"] = df_learner_dataset[\"neg_log_loss\"].apply(lambda score: regret(score))\n",
    "\n",
    "        # fill the remaining spots, without recommendations with worst regret score: 1\n",
    "        for i in range(len(df_learner_dataset), n_recommendation):\n",
    "            df_learner_dataset.loc[i] = [metalearner, did, missing_entry_val, 1]\n",
    "        \n",
    "        # compute potential for metalearner on this dataset\n",
    "        metalearner_dataset_potentials.append(df_learner_dataset[\"potential\"].mean())\n",
    "        metalearner_dataset_potentials_std.append(df_learner_dataset[\"potential\"].std())\n",
    "\n",
    "    mtl_potential = sum(metalearner_dataset_potentials)/len(metalearner_dataset_potentials)\n",
    "    mtl_std = sum(metalearner_dataset_potentials_std)/len(metalearner_dataset_potentials_std)\n",
    "    metalearner_potential[metalearner] = mtl_potential\n",
    "    metalearner_std[metalearner] = mtl_std\n",
    "    print(f\"metalearner {metalearner} has a potential score of {metalearner_potential}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner_potential # regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner_std # standard deviation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and potential scores for OpenML-CC18 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time_restricted_results_partial_openmlcc18.csv\n",
    "df_all = pd.read_csv(\"time_restricted_results.csv\")\n",
    "openmlcc18_subset_dids = [59, 47, 31, 71, 25, 26, 29, 32, 28, 33, 60, 58, 21, 39, 30, 70, 4, 6, 8, 65, 3, 49, 64, 52, 46, 19, 56, 66, 42, 45, 43, 24, 35, 53, 27, 63, 67, 68]\n",
    "df_all = df_all[df_all[\"dataset_id\"].isin(openmlcc18_subset_dids)]\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "df_all.to_csv(\"time_restricted_results_partial_openmlcc18.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out getting the potential score\n",
    "df = pd.read_csv(\"time_restricted_results_partial_openmlcc18.csv\")\n",
    "\n",
    "metalearners = df[\"metalearner_name\"].unique().tolist()\n",
    "dataset_ids = df[\"dataset_id\"].unique().tolist()\n",
    "\n",
    "n_recommendation = 25\n",
    "\n",
    "def regret(score):\n",
    "    if score < 100000: # filter out NAN\n",
    "        return abs(max_score - score) / abs(min_score - max_score)\n",
    "    return 1\n",
    "\n",
    "idx_to_keep = []\n",
    "rows_to_add = []\n",
    "metalearner_potential = {}\n",
    "metalearner_std = {}\n",
    "for metalearner in metalearners:\n",
    "    metalearner_dataset_potentials = []\n",
    "    metalearner_dataset_potentials_std = []\n",
    "    for did in dataset_ids:\n",
    "        # filter out nan with selection: < 1000000\n",
    "        missing_entry_val = min(df[(df[\"dataset_id\"] == did) & (df[\"neg_log_loss\"] < 1000000)][\"neg_log_loss\"]) - 0.01\n",
    "        df_did = df[df[\"dataset_id\"] == did]\n",
    "        did_losses_without_nan = [df_did.iloc[i][\"neg_log_loss\"] for i in range(0, len(df_did)) if df_did.iloc[i][\"neg_log_loss\"] < 1_000_000] # comparison to exclude nans\n",
    "        min_score = min(did_losses_without_nan)  \n",
    "        max_score = max(did_losses_without_nan) \n",
    "\n",
    "        df_learner_dataset = df[(df[\"metalearner_name\"] == metalearner) & (df[\"dataset_id\"] == did)]\n",
    "        df_learner_dataset[\"potential\"] = df_learner_dataset[\"neg_log_loss\"].apply(lambda score: regret(score))\n",
    "\n",
    "        # fill the remaining spots, without recommendations with worst regret score: 1\n",
    "        for i in range(len(df_learner_dataset), n_recommendation):\n",
    "            df_learner_dataset.loc[i] = [metalearner, did, missing_entry_val, 1]\n",
    "        \n",
    "        # compute potential for metalearner on this dataset\n",
    "        metalearner_dataset_potentials.append(df_learner_dataset[\"potential\"].mean())\n",
    "        metalearner_dataset_potentials_std.append(df_learner_dataset[\"potential\"].std())\n",
    "\n",
    "    mtl_potential = sum(metalearner_dataset_potentials)/len(metalearner_dataset_potentials)\n",
    "    mtl_std = sum(metalearner_dataset_potentials_std)/len(metalearner_dataset_potentials_std)\n",
    "    metalearner_potential[metalearner] = mtl_potential\n",
    "    metalearner_std[metalearner] = mtl_std\n",
    "    print(f\"metalearner {metalearner} has a potential score of {metalearner_potential}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial openml-cc18 regret\n",
    "metalearner_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial openml-cc18 std\n",
    "metalearner_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Rank distribution plots time-restricted analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full OpenML-CC18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"time_restricted_results_no_dataset2vec_processed_with_baselines.csv\")\n",
    "results_df[\"neg_log_loss\"] = results_df[\"neg_log_loss\"].astype(float)\n",
    "\n",
    "all_ranks = []\n",
    "for did in results_df[\"dataset_id\"].unique().tolist():\n",
    "    did_df = results_df[results_df[\"dataset_id\"] == did]\n",
    "    metalearners_ranked = did_df.sort_values(by=\"neg_log_loss\", ascending=False)[\"metalearner_name\"].to_list()\n",
    "    dataset_ranks = [(metalearners_ranked[rank - 1], rank) for rank in range(1, len(metalearners_ranked) + 1)]\n",
    "    all_ranks.append(dataset_ranks)\n",
    "\n",
    "all_ranks\n",
    "ranks_per_metalearner = [] # get the ranks now grouped by each meta-learner\n",
    "metalearner_names = results_df[\"metalearner_name\"].unique().tolist()\n",
    "for metalearner in metalearner_names:\n",
    "    metalearner_ranks = []\n",
    "    for did_ranks in all_ranks:\n",
    "        for rank_entry in did_ranks:\n",
    "            if rank_entry[0] == metalearner:\n",
    "                metalearner_ranks.append(rank_entry[1])\n",
    "    ranks_per_metalearner.append(metalearner_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner_names = ['ASHA',\n",
    " 'RandomSearch',\n",
    " 'AsyncEA',\n",
    " 'TopSimilarity-Feurer',\n",
    " 'TopSimilarity-Wistuba',\n",
    " 'AP-Feurer',\n",
    " 'AP-Wistuba',\n",
    " 'Rankml-Feurer',\n",
    " 'Rankml-Wistuba',\n",
    " 'UtilityEstimate-Feurer',\n",
    " 'UtilityEstimate-Wistuba',\n",
    " 'PortfolioBuilding',\n",
    " 'AverageRegret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(2)\n",
    "\n",
    "plt.violinplot(ranks_per_metalearner, showmeans=True, widths=0.8) # labels=metalearner_names\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Dataset ranks\")\n",
    "ax.set_xticklabels(metalearner_names)\n",
    "ax.set_xticks([i + 1 for i in range(0, len(metalearner_names))])\n",
    "ax.tick_params(axis='x', labelrotation = -70)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial OpenML-CC18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"time_restricted_results_no_dataset2vec_processed_with_baselines.csv\")\n",
    "results_df[\"neg_log_loss\"] = results_df[\"neg_log_loss\"].astype(float)\n",
    "\n",
    "all_ranks = []\n",
    "for did in results_df[\"dataset_id\"].unique().tolist():\n",
    "    did_df = results_df[results_df[\"dataset_id\"] == did]\n",
    "    metalearners_ranked = did_df.sort_values(by=\"neg_log_loss\", ascending=False)[\"metalearner_name\"].to_list()\n",
    "    dataset_ranks = [(metalearners_ranked[rank - 1], rank) for rank in range(1, len(metalearners_ranked) + 1)]\n",
    "    all_ranks.append(dataset_ranks)\n",
    "\n",
    "all_ranks\n",
    "ranks_per_metalearner = [] # get the ranks now grouped by each meta-learner\n",
    "metalearner_names = results_df[\"metalearner_name\"].unique().tolist()\n",
    "for metalearner in metalearner_names:\n",
    "    metalearner_ranks = []\n",
    "    for did_ranks in all_ranks:\n",
    "        for rank_entry in did_ranks:\n",
    "            if rank_entry[0] == metalearner:\n",
    "                metalearner_ranks.append(rank_entry[1])\n",
    "    ranks_per_metalearner.append(metalearner_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metalearner_names = ['ASHA',\n",
    " 'RandomSearch',\n",
    " 'AsyncEA',\n",
    " 'TopSimilarity-Feurer',\n",
    " 'TopSimilarity-Wistuba',\n",
    " 'TopSimilarity-D2V',\n",
    " 'AP-Feurer',\n",
    " 'AP-Wistuba',\n",
    " 'AP-D2V',\n",
    " 'AP-D2V',\n",
    " 'Rankml-Feurer',\n",
    " 'Rankml-Wistuba',\n",
    " 'Rankml-D2V',\n",
    " 'UtilityEstimate-Feurer',\n",
    " 'UtilityEstimate-Wistuba',\n",
    " 'UtilityEstimate-D2V',\n",
    " 'PortfolioBuilding',\n",
    " 'AverageRegret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(12)\n",
    "f.set_figheight(2)\n",
    "\n",
    "plt.violinplot(ranks_per_metalearner, showmeans=True, widths=0.8) # labels=metalearner_names\n",
    "ax = plt.gca()\n",
    "ax.set_ylabel(\"Dataset ranks\")\n",
    "ax.set_xticklabels(metalearner_names)\n",
    "ax.set_xticks([i + 1 for i in range(0, len(metalearner_names))])\n",
    "ax.tick_params(axis='x', labelrotation = -70)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings for the dataset characterizations measures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process the available timings, e.g. get in better format to create plots from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# process timings\n",
    "filename = \"feurer_timings_raw.txt\"\n",
    "with open(filename) as file:\n",
    "    lines = [line.rstrip().split(\" at \")[1] for line in file]\n",
    "feurer_datetimes = [datetime.strptime(line, '%Y-%m-%d %H:%M:%S.%f') for line in lines]\n",
    "feurer_time_difs = [(feurer_datetimes[i + 1] - feurer_datetimes[i]).total_seconds() for i in range(0, len(feurer_datetimes) - 1)]\n",
    "feurer_dataset_timings = {}\n",
    "for i, time_dif in enumerate(feurer_time_difs):\n",
    "    feurer_dataset_timings[i] = time_dif\n",
    "\n",
    "filename = \"wistuba_timings_raw.txt\"\n",
    "with open(filename) as file:\n",
    "    lines = [line.rstrip().split(\" at \")[1] for line in file]\n",
    "wistuba_datetimes = [datetime.strptime(line, '%Y-%m-%d %H:%M:%S.%f') for line in lines]\n",
    "wistuba_time_difs = [(wistuba_datetimes[i + 1] - wistuba_datetimes[i]).total_seconds() for i in range(0, len(wistuba_datetimes) - 1)]\n",
    "wistuba_dataset_timings = {}\n",
    "for i, time_dif in enumerate(wistuba_time_difs):\n",
    "    wistuba_dataset_timings[i] = time_dif\n",
    "\n",
    "filename = \"dataset2vec_timings_raw.txt\"\n",
    "with open(filename) as file:\n",
    "    lines = [line.rstrip().split(\" at \")[1] for line in file]\n",
    "d2v_datetimes = [datetime.strptime(line, '%Y-%m-%d %H:%M:%S.%f') for line in lines]\n",
    "d2v_time_difs = [(d2v_datetimes[i + 1] - d2v_datetimes[i]).total_seconds() for i in range(0, len(d2v_datetimes) - 1)]\n",
    "d2v_dataset_timings = {}\n",
    "for i, time_dif in enumerate(d2v_time_difs):\n",
    "    d2v_dataset_timings[i] = time_dif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets ordered by samples, features, outcomes. Needed for plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets ordered by samples, features, outcomes\n",
    "did_n_samples =  {71: 500, 31: 522, 50: 540, 59: 540, 36: 569, 41: 583, 2: 625, 11: 690, 5: 699, 21: 736, 48: 748, 14: 768, 26: 797, 25: 841, 18: 846, 17: 958, 23: 990, 12: 1000, 37: 1055, 45: 1080, 58: 1080, 33: 1109, 47: 1372, 28: 1458, 9: 1473, 29: 1563, 40: 1593, 61: 1728, 57: 1941, 3: 2000, 4: 2000, 6: 2000, 7: 2000, 8: 2000, 65: 2000, 32: 2109, 62: 2310, 44: 2534, 42: 2600, 69: 3186, 16: 3190, 0: 3196, 56: 3279, 35: 3751, 22: 3772, 15: 4601, 60: 4839, 70: 5000, 39: 5404, 38: 5456, 54: 5500, 10: 5620, 46: 6118, 20: 6430, 24: 7797, 52: 9873, 53: 10299, 30: 10885, 13: 10992, 49: 11055, 1: 20000, 43: 34465, 64: 44819, 51: 45211, 19: 45312, 34: 48842, 68: 60000, 55: 67557, 27: 70000, 63: 70000, 67: 92000, 66: 96320}\n",
    "did_n_features =  {2: 4, 26: 4, 47: 4, 48: 4, 39: 5, 60: 5, 7: 6, 61: 6, 64: 6, 14: 8, 19: 8, 5: 9, 9: 9, 17: 9, 41: 10, 23: 12, 71: 12, 34: 14, 11: 15, 1: 16, 13: 16, 51: 16, 18: 18, 21: 19, 62: 19, 12: 20, 59: 20, 70: 20, 30: 21, 31: 21, 32: 21, 33: 21, 66: 21, 38: 24, 57: 27, 22: 29, 36: 30, 49: 30, 52: 32, 0: 36, 20: 36, 28: 37, 29: 37, 50: 39, 54: 40, 37: 41, 55: 42, 8: 47, 46: 51, 15: 57, 16: 61, 6: 64, 10: 64, 25: 70, 44: 72, 4: 76, 58: 81, 43: 118, 69: 180, 3: 216, 65: 240, 40: 256, 42: 500, 53: 561, 24: 617, 27: 784, 63: 784, 45: 856, 67: 1024, 56: 1558, 35: 1776, 68: 3072}\n",
    "did_n_outcomes =  {0: 2, 5: 2, 11: 2, 12: 2, 14: 2, 15: 2, 17: 2, 19: 2, 22: 2, 28: 2, 29: 2, 30: 2, 31: 2, 32: 2, 33: 2, 34: 2, 35: 2, 36: 2, 37: 2, 39: 2, 41: 2, 42: 2, 43: 2, 44: 2, 47: 2, 48: 2, 49: 2, 50: 2, 51: 2, 56: 2, 59: 2, 60: 2, 66: 2, 70: 2, 71: 2, 2: 3, 9: 3, 16: 3, 55: 3, 64: 3, 69: 3, 18: 4, 25: 4, 38: 4, 61: 4, 21: 5, 52: 5, 65: 5, 20: 6, 26: 6, 46: 6, 53: 6, 57: 7, 62: 7, 58: 8, 45: 9, 3: 10, 4: 10, 6: 10, 7: 10, 8: 10, 10: 10, 13: 10, 27: 10, 40: 10, 63: 10, 68: 10, 23: 11, 54: 11, 1: 26, 24: 26, 67: 46}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually create the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, sharex=False, sharey=False, figsize=(10, 8))\n",
    "\n",
    "import numpy as np\n",
    "x = np.random.rand(10)\n",
    "y = np.random.rand(10)\n",
    "z = np.sqrt(x**2 + y**2)\n",
    "\n",
    "# D2V n_samples\n",
    "x = list(did_n_samples.values())\n",
    "y = [d2v_dataset_timings[did] for did in did_n_samples.keys()]\n",
    "axs[0, 0].scatter(x, y, color=\"tab:blue\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[0, 0].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[0, 0].set_xticks([])\n",
    "for i, id in enumerate(list(did_n_samples.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 56 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[0, 0].annotate(\" \" + str(id), (x[i], y[i] - 7))\n",
    "        else:\n",
    "            axs[0, 0].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[0, 0].set_title(\"\\n # Samples\")\n",
    "axs[0, 0].set_ylabel(\"time (seconds)\")\n",
    "\n",
    "# D2V n_features\n",
    "x = list(did_n_features.values())\n",
    "y = [d2v_dataset_timings[did] for did in did_n_features.keys()]\n",
    "axs[0, 1].scatter(x, y, color=\"tab:orange\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[0, 1].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[0, 1].set_xticks([])\n",
    "axs[0, 1].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_features.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 56 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[0, 1].annotate(\" \" + str(id), (x[i], y[i] - 7))\n",
    "        else:\n",
    "            axs[0, 1].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[0, 1].set_title(\"Dataset2Vec \\n# Features\")\n",
    "\n",
    "# D2V n_outcomes\n",
    "x = list(did_n_outcomes.values())\n",
    "y = [d2v_dataset_timings[did] for did in did_n_outcomes.keys()]\n",
    "axs[0, 2].scatter(x, y, color=\"tab:green\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[0, 2].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[0, 2].set_title(\"\\n # Outcomes\")\n",
    "axs[0, 2].set_xticks([])\n",
    "axs[0, 2].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_outcomes.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 56 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[0, 2].annotate(\" \" + str(id), (x[i], y[i] - 7))\n",
    "        else:\n",
    "            axs[0, 2].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "\n",
    "# Feurer n_samples\n",
    "x = list(did_n_samples.values())\n",
    "y = [feurer_dataset_timings[did] for did in did_n_samples.keys()]\n",
    "axs[1, 0].scatter(x, y, color=\"tab:blue\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[1, 0].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[1, 0].set_xticks([])\n",
    "for i, id in enumerate(list(did_n_samples.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[1, 0].annotate(\" \" + str(id), (x[i], y[i] - 1000))\n",
    "        else:\n",
    "            axs[1, 0].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[1, 0].set_ylabel(\"time (seconds)\")\n",
    "\n",
    "# Feurer n_features\n",
    "x = list(did_n_features.values())\n",
    "y = [feurer_dataset_timings[did] for did in did_n_features.keys()]\n",
    "axs[1, 1].scatter(x, y, color=\"tab:orange\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[1, 1].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[1, 1].set_xticks([])\n",
    "axs[1, 1].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_features.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[1, 1].annotate(\" \" + str(id), (x[i], y[i] - 1000))\n",
    "        else:\n",
    "            axs[1, 1].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[1, 1].set_title(\"Feurer MF\")\n",
    "\n",
    "# Feurer n_outcomes\n",
    "x = list(did_n_outcomes.values())\n",
    "y = [feurer_dataset_timings[did] for did in did_n_outcomes.keys()]\n",
    "axs[1, 2].scatter(x, y, color=\"tab:green\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[1, 2].set_xticks([])\n",
    "axs[1, 2].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_outcomes.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[1, 2].annotate(\" \" + str(id), (x[i], y[i] - 1000))\n",
    "        else:\n",
    "            axs[1, 2].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[1, 2].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "\n",
    "# Wistuba n_samples\n",
    "x = list(did_n_samples.values())\n",
    "y = [wistuba_dataset_timings[did] for did in did_n_samples.keys()]\n",
    "axs[2, 0].scatter(x, y, color=\"tab:blue\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[2, 0].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "for i, id in enumerate(list(did_n_samples.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[2, 0].annotate(\" \" + str(id), (x[i], y[i] - 50))\n",
    "        else:\n",
    "            axs[2, 0].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[2, 0].set_ylabel(\"time (seconds)\")\n",
    "\n",
    "# Wistuba n_features\n",
    "x = list(did_n_features.values())\n",
    "y = [wistuba_dataset_timings[did] for did in did_n_features.keys()]\n",
    "axs[2, 1].scatter(x, y, color=\"tab:orange\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[2, 1].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[2, 1].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_features.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[2, 1].annotate(\" \" + str(id), (x[i], y[i] - 50))\n",
    "        else:\n",
    "            axs[2, 1].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[2, 1].set_title(\"Wistuba MF\")\n",
    "\n",
    "# Wistuba n_outcomes\n",
    "x = list(did_n_outcomes.values())\n",
    "y = [wistuba_dataset_timings[did] for did in did_n_outcomes.keys()]\n",
    "axs[2, 2].scatter(x, y, color=\"tab:green\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[2, 2].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[2, 2].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_outcomes.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[2, 2].annotate(\" \" + str(id), (x[i], y[i] - 50))\n",
    "        else:\n",
    "            axs[2, 2].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets ordered by samples, features, outcomes. But now excluding the annotated datasets (e.g. the outliers) Needed for plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets ordered by samples, features, outcomes\n",
    "did_n_samples =  {71: 500, 31: 522, 50: 540, 59: 540, 36: 569, 41: 583, 2: 625, 11: 690, 5: 699, 21: 736, 48: 748, 14: 768, 26: 797, 25: 841, 18: 846, 17: 958, 23: 990, 12: 1000, 37: 1055, 45: 1080, 58: 1080, 33: 1109, 47: 1372, 28: 1458, 9: 1473, 29: 1563, 40: 1593, 61: 1728, 57: 1941, 3: 2000, 4: 2000, 6: 2000, 7: 2000, 8: 2000, 65: 2000, 32: 2109, 62: 2310, 44: 2534, 42: 2600, 69: 3186, 16: 3190, 0: 3196, 35: 3751, 22: 3772, 15: 4601, 60: 4839, 70: 5000, 39: 5404, 38: 5456, 54: 5500, 10: 5620, 46: 6118, 20: 6430, 24: 7797, 52: 9873, 53: 10299, 30: 10885, 13: 10992, 49: 11055, 1: 20000, 43: 34465, 64: 44819, 51: 45211, 19: 45312, 34: 48842, 55: 67557, 66: 96320}\n",
    "did_n_features =  {2: 4, 26: 4, 47: 4, 48: 4, 39: 5, 60: 5, 7: 6, 61: 6, 64: 6, 14: 8, 19: 8, 5: 9, 9: 9, 17: 9, 41: 10, 23: 12, 71: 12, 34: 14, 11: 15, 1: 16, 13: 16, 51: 16, 18: 18, 21: 19, 62: 19, 12: 20, 59: 20, 70: 20, 30: 21, 31: 21, 32: 21, 33: 21, 66: 21, 38: 24, 57: 27, 22: 29, 36: 30, 49: 30, 52: 32, 0: 36, 20: 36, 28: 37, 29: 37, 50: 39, 54: 40, 37: 41, 55: 42, 8: 47, 46: 51, 15: 57, 16: 61, 6: 64, 10: 64, 25: 70, 44: 72, 4: 76, 58: 81, 43: 118, 69: 180, 3: 216, 65: 240, 40: 256, 42: 500, 53: 561, 24: 617, 45: 856, 35: 1776}\n",
    "did_n_outcomes =  {0: 2, 5: 2, 11: 2, 12: 2, 14: 2, 15: 2, 17: 2, 19: 2, 22: 2, 28: 2, 29: 2, 30: 2, 31: 2, 32: 2, 33: 2, 34: 2, 35: 2, 36: 2, 37: 2, 39: 2, 41: 2, 42: 2, 43: 2, 44: 2, 47: 2, 48: 2, 49: 2, 50: 2, 51: 2, 59: 2, 60: 2, 66: 2, 70: 2, 71: 2, 2: 3, 9: 3, 16: 3, 55: 3, 64: 3, 69: 3, 18: 4, 25: 4, 38: 4, 61: 4, 21: 5, 52: 5, 65: 5, 20: 6, 26: 6, 46: 6, 53: 6, 57: 7, 62: 7, 58: 8, 45: 9, 3: 10, 4: 10, 6: 10, 7: 10, 8: 10, 10: 10, 13: 10, 40: 10, 23: 11, 54: 11, 1: 26, 24: 26}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the plot without the annotated datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, sharex=False, sharey=False, figsize=(10, 8))\n",
    "\n",
    "import numpy as np\n",
    "x = np.random.rand(10)\n",
    "y = np.random.rand(10)\n",
    "z = np.sqrt(x**2 + y**2)\n",
    "\n",
    "# D2V n_samples\n",
    "x = list(did_n_samples.values())\n",
    "y = [d2v_dataset_timings[did] for did in did_n_samples.keys()]\n",
    "axs[0, 0].scatter(x, y, color=\"tab:blue\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[0, 0].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[0, 0].set_xticks([])\n",
    "for i, id in enumerate(list(did_n_samples.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 56 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[0, 0].annotate(\" \" + str(id), (x[i], y[i] - 7))\n",
    "        else:\n",
    "            axs[0, 0].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[0, 0].set_title(\"\\n # Samples\")\n",
    "axs[0, 0].set_ylabel(\"time (seconds)\")\n",
    "\n",
    "# D2V n_features\n",
    "x = list(did_n_features.values())\n",
    "y = [d2v_dataset_timings[did] for did in did_n_features.keys()]\n",
    "axs[0, 1].scatter(x, y, color=\"tab:orange\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[0, 1].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[0, 1].set_xticks([])\n",
    "axs[0, 1].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_features.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 56 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[0, 1].annotate(\" \" + str(id), (x[i], y[i] - 7))\n",
    "        else:\n",
    "            axs[0, 1].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[0, 1].set_title(\"Dataset2Vec \\n# Features\")\n",
    "\n",
    "# D2V n_outcomes\n",
    "x = list(did_n_outcomes.values())\n",
    "y = [d2v_dataset_timings[did] for did in did_n_outcomes.keys()]\n",
    "axs[0, 2].scatter(x, y, color=\"tab:green\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[0, 2].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[0, 2].set_title(\"\\n # Outcomes\")\n",
    "axs[0, 2].set_xticks([])\n",
    "axs[0, 2].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_outcomes.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 56 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[0, 2].annotate(\" \" + str(id), (x[i], y[i] - 7))\n",
    "        else:\n",
    "            axs[0, 2].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "\n",
    "# Feurer n_samples\n",
    "x = list(did_n_samples.values())\n",
    "y = [feurer_dataset_timings[did] for did in did_n_samples.keys()]\n",
    "axs[1, 0].scatter(x, y, color=\"tab:blue\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[1, 0].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[1, 0].set_xticks([])\n",
    "for i, id in enumerate(list(did_n_samples.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[1, 0].annotate(\" \" + str(id), (x[i], y[i] - 1000))\n",
    "        else:\n",
    "            axs[1, 0].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[1, 0].set_ylabel(\"time (seconds)\")\n",
    "\n",
    "# Feurer n_features\n",
    "x = list(did_n_features.values())\n",
    "y = [feurer_dataset_timings[did] for did in did_n_features.keys()]\n",
    "axs[1, 1].scatter(x, y, color=\"tab:orange\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[1, 1].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[1, 1].set_xticks([])\n",
    "axs[1, 1].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_features.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[1, 1].annotate(\" \" + str(id), (x[i], y[i] - 1000))\n",
    "        else:\n",
    "            axs[1, 1].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[1, 1].set_title(\"Feurer MF\")\n",
    "\n",
    "# Feurer n_outcomes\n",
    "x = list(did_n_outcomes.values())\n",
    "y = [feurer_dataset_timings[did] for did in did_n_outcomes.keys()]\n",
    "axs[1, 2].scatter(x, y, color=\"tab:green\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[1, 2].set_xticks([])\n",
    "axs[1, 2].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_outcomes.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        if id != 63:\n",
    "            axs[1, 2].annotate(\" \" + str(id), (x[i], y[i] - 1000))\n",
    "        else:\n",
    "            axs[1, 2].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[1, 2].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "\n",
    "# Wistuba n_samples\n",
    "x = list(did_n_samples.values())\n",
    "y = [wistuba_dataset_timings[did] for did in did_n_samples.keys()]\n",
    "axs[2, 0].scatter(x, y, color=\"tab:blue\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[2, 0].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "for i, id in enumerate(list(did_n_samples.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        axs[2, 0].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[2, 0].set_ylabel(\"time (seconds)\")\n",
    "\n",
    "# Wistuba n_features\n",
    "x = list(did_n_features.values())\n",
    "y = [wistuba_dataset_timings[did] for did in did_n_features.keys()]\n",
    "axs[2, 1].scatter(x, y, color=\"tab:orange\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[2, 1].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[2, 1].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_features.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        axs[2, 1].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "axs[2, 1].set_title(\"Wistuba MF\")\n",
    "\n",
    "# Wistuba n_outcomes\n",
    "x = list(did_n_outcomes.values())\n",
    "y = [wistuba_dataset_timings[did] for did in did_n_outcomes.keys()]\n",
    "axs[2, 2].scatter(x, y, color=\"tab:green\")\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "xseq = np.linspace(0, max(x), num=100)\n",
    "axs[2, 2].plot(xseq, a + b * xseq, color=\"k\", lw=1) # Plot regression line\n",
    "axs[2, 2].set_yticks([])\n",
    "for i, id in enumerate(list(did_n_outcomes.keys())):\n",
    "    if id == 68 or id == 67 or id == 63 or id == 27:\n",
    "        axs[2, 2].annotate(\" \" + str(id), (x[i], y[i]))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for warm-starting analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_gama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
